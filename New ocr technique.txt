Objective: Replace parser-based PDF extraction with a model-based pipeline using a local VLM via Ollama. Default to Qwen2.5-VL-7B-Instruct for page-image transcription (verbatim + light structure). Provide a lightweight OCR fallback (PaddleOCR-VL / PP-OCRv5) when confidence is low. Keep everything local, deterministic, and replayable.

Models & Runtime

Primary VLM: Qwen2.5-VL-7B-Instruct (via Ollama if available; else local runner/HTTP microservice). Use it for page-image → verbatim text with minimal structure tags.

Fallback OCR: PaddleOCR-VL (0.9B) or PP-OCRv5 (local Python service). Only engage on low-confidence pages.

Embeddings: Keep our current Ollama embedding model.

Chat model: Keep our current Ollama chat model.

Document these choices in README (“Model Contract: names, dims, VRAM, expected latency”).
(Refs: Qwen2.5-VL doc parsing & OCR gains; PaddleOCR updates; Nougat for academic PDFs; DeepSeek-OCR as emerging option). 
The Times of India
+8
arXiv
+8
Qwen
+8

Ingestion Flow (VLM-First)

Rasterize pages at 300–400 DPI PNG (lossless). Keep page numbers and an image hash per page.

VLM Transcription (default path):

Prompt the VLM to output verbatim text with light tags only: <H1>, <H2>, <P>, <BULLET>, <TABLE md>.

No paraphrase, no hallucination, no “fixing” text; preserve reading order.

Output must be strict JSON:

{page, char_count, blocks: [...], raw_markdown} (exact schema defined in our spec).

Enforce deterministic settings (low temperature, fixed seed).

Store the input image hash, exact prompt, model name, seed, and the raw JSON for replay/audit.

Confidence Gate:

Compute cheap heuristics: char_count vs expected density (by page area), OCR-like token diversity, proportion of alphanumerics, presence of repeated garbage characters.

If below threshold or raw_markdown too short for a dense page → trigger Fallback OCR for that page.

Fallback OCR (page-level):

Run PaddleOCR-VL / PP-OCRv5 locally; produce Markdown-like text with table hints if possible.

Mark metadata extraction_method = fallback_ocr.

Chunking:

From raw_markdown, make ~800-token chunks with small overlaps; attach page ranges (p.12 or p.12–13).

Proceed with embeddings and indexing exactly as before.

Health & Readiness (extend existing checks)

Ollama VLM availability: verify the configured VLM is present and loadable. If not, print the exact pull command and fail early.

Rasterizer present: verify the PDF→PNG tool works (and DPI is honored).

Fallback OCR presence: verify PaddleOCR service is reachable (or clearly marked optional).

/health must expose these checks with PASS/FAIL and human-readable fixes.

Determinism & JSON Discipline

Force VLM JSON-only outputs; perform one constrained retry if JSON invalid.

Save the raw model text on failure for debugging.

Record seed, temperature, model, and prompt_version with every page result.

Citations & Storage

Use page ranges for citations (since chunks are derived from page-level text).

Persist: page_image_path, page_hash, extraction_method (vlm|fallback), confidence, and the exact JSON payload.

On replay: re-rasterize (optional), re-run the same model with the same seed; compare hashes to ensure fidelity.

Evals (add to our existing eval harness)

Page-level fidelity on a small labeled subset: character-level accuracy or word error rate (WER).

Structure presence: % pages with detected headings/bullets/tables where applicable.

Confidence calibration: low-confidence pages should correlate with higher WER (spot-check).

Continue our RAG metrics (Attribution@K, No-Answer accuracy, p50/p95 latency).

Performance & Resource Guidelines

Batch page images where possible; cap max tokens per page for the VLM.

Use quantized VLM if VRAM is tight; allow a config flag to drop DPI to 300 on slow machines.

Use fallback only when needed; log the fallback rate.

Security & Privacy

All images and model outputs remain on disk locally; no external calls.

Add a “redact on export” option if users share page snippets in issues.

Rollout Order (so you don’t chase ghosts)

Rasterize only; prove page image list with hashes.

VLM single-page path: run one page through VLM, get valid JSON.

Multi-page batch with confidence scoring; no fallback yet.

Enable fallback and verify it triggers only on low-confidence pages.

Chunk → embed → retrieve remains unchanged; confirm citations show p.N.

Add the page-level evals and update README with model names and toggles.

Troubleshooting (standard responses)

VLM missing/unloadable → show ollama pull <qwen2.5-vl-7b-instruct> (or local runner instructions).

Slow transcription → lower DPI to 300, use quantized VLM, increase batch size cautiously, or rely more on fallback OCR.

Garbage output → increase DPI for that page and/or trigger fallback OCR.

JSON invalid → one retry; if still invalid, store raw and mark the page for re-ingest review.

Implement now. Prioritize fidelity (verbatim text), JSON reliability, and page-accurate citations. Keep classic parsers disabled for PDFs in v1 of this new path.